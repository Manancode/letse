import torch
import torch.nn as nn
import librosa
import numpy as np
# REMOVED: mir_eval.separation import - VoiceFilter-Lite 2020 doesn't use SDR!
from .audio import Audio


def asymmetric_l2_loss(pred, target, alpha):
    """
    VoiceFilter-Lite 2020 Asymmetric L2 Loss
    Same as in train.py for consistency
    """
    diff = target - pred
    gasym_diff = torch.where(diff <= 0, diff, alpha * diff)
    loss = gasym_diff ** 2
    return loss.mean()


def validate(audio, model, embedder, testloader, writer, step, hp):
    """
    VoiceFilter-Lite 2020: Feature-domain validation only
    
    GOOGLE SLIDES: "Word Error Rate (WER) is all we need"
    NO SDR/SNR metrics since VoiceFilter-Lite outputs ASR features only
    """
    model.eval()
    
    # VoiceFilter-Lite 2020: Use asymmetric L2 loss for validation
    alpha = hp.train.asymmetric_loss_alpha
    
    total_separation_loss = 0.0
    total_noise_pred_loss = 0.0
    total_enhanced_feature_quality = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for batch in testloader:
            # VoiceFilter-Lite 2020: Handle features instead of spectrograms
            dvec_mel, target_wav, mixed_wav, target_features, mixed_features, mixed_phase = batch[0]

            dvec_mel = dvec_mel.cuda()
            target_features = target_features.unsqueeze(0).cuda()
            mixed_features = mixed_features.unsqueeze(0).cuda()

            dvec = embedder(dvec_mel)
            dvec = dvec.unsqueeze(0)
            
            # VoiceFilter-Lite 2020: Get dual outputs
            mask, noise_pred = model(mixed_features, dvec)
            
            # Apply mask to get enhanced features
            enhanced_features = model._apply_mask_to_stacked_features(mixed_features, mask)
            
            # VoiceFilter-Lite 2020: Asymmetric L2 loss for validation
            separation_loss = asymmetric_l2_loss(enhanced_features, target_features, alpha)
            
            # Noise prediction loss with proper labels
            batch_size, time_steps, _ = target_features.shape
            # Generate realistic noise labels (speech vs non-speech)
            noise_labels = generate_validation_noise_labels(target_features, mixed_features)
            criterion = nn.CrossEntropyLoss()
            noise_pred_flat = noise_pred.view(-1, noise_pred.size(-1))
            noise_labels_flat = noise_labels.view(-1)
            noise_loss = criterion(noise_pred_flat, noise_labels_flat)
            
            # Feature quality metrics (VoiceFilter-Lite 2020 specific)
            feature_snr = compute_feature_snr(enhanced_features, target_features, mixed_features)
            
            total_separation_loss += separation_loss.item()
            total_noise_pred_loss += noise_loss.item()
            total_enhanced_feature_quality += feature_snr
            num_batches += 1
            
            # VoiceFilter-Lite 2020: NO AUDIO RECONSTRUCTION!
            # The paper explicitly states: "we are not going to report SNR/SDR metrics"
            
            break  # Only validate on first batch for speed

    # Calculate average losses
    avg_separation_loss = total_separation_loss / max(num_batches, 1)
    avg_noise_loss = total_noise_pred_loss / max(num_batches, 1)
    avg_feature_quality = total_enhanced_feature_quality / max(num_batches, 1)
    
    # Log VoiceFilter-Lite 2020 metrics (NO SDR!)
    if hasattr(writer, 'log_validation_detailed'):
        writer.log_validation_detailed({
            'separation_loss': avg_separation_loss,
            'noise_prediction_loss': avg_noise_loss,
            'feature_snr': avg_feature_quality,  # Feature-domain metric
        }, step)
    
    print(f"VoiceFilter-Lite 2020 Validation:")
    print(f"  Separation Loss: {avg_separation_loss:.4f}")
    print(f"  Noise Loss: {avg_noise_loss:.4f}")
    print(f"  Feature SNR: {avg_feature_quality:.2f} dB")
    print(f"  üìä For WER evaluation, use enhanced features with ASR system")

    # Restore model to training mode
    model.train()


def generate_validation_noise_labels(target_features, mixed_features):
    """
    Generate realistic noise type labels for validation
    Based on energy difference between target and mixed features
    """
    batch_size, time_steps, _ = target_features.shape
    
    # Calculate feature energy (sum of squares across feature dimension)
    target_energy = torch.sum(target_features**2, dim=-1)  # [B, T]
    mixed_energy = torch.sum(mixed_features**2, dim=-1)    # [B, T]
    
    # Energy ratio indicates interference level
    energy_ratio = mixed_energy / (target_energy + 1e-8)
    
    # Label 0: Clean/non-speech noise, Label 1: Overlapped speech
    # Higher ratio suggests speech interference
    noise_labels = (energy_ratio > 1.3).long()  # Adjusted threshold
    
    return noise_labels


def compute_feature_snr(enhanced_features, target_features, mixed_features):
    """
    VoiceFilter-Lite 2020: Feature-domain SNR (not audio SNR!)
    Measures enhancement quality in feature space
    """
    # Signal: target features
    signal_power = torch.mean(target_features ** 2)
    
    # Noise: difference between enhanced and target
    noise = enhanced_features - target_features
    noise_power = torch.mean(noise ** 2)
    
    # Feature SNR in dB
    feature_snr = 10 * torch.log10(signal_power / (noise_power + 1e-8))
    
    return feature_snr.item()


def evaluate_with_asr(model, embedder, test_data, asr_system, hp):
    """
    VoiceFilter-Lite 2020: Proper evaluation with ASR system
    
    This is how you should evaluate your model:
    1. Enhanced features ‚Üí ASR ‚Üí Transcription ‚Üí WER
    NOT: Enhanced features ‚Üí Audio ‚Üí SDR
    """
    model.eval()
    
    total_wer_improvement = 0.0
    num_samples = 0
    
    with torch.no_grad():
        for batch in test_data:
            dvec_mel, target_wav, mixed_wav, target_features, mixed_features, _ = batch
            
            # Get enhanced features
            dvec = embedder(dvec_mel.cuda())
            enhanced_features, noise_pred = model.separate_speech(
                mixed_features.cuda(), dvec.unsqueeze(0), 
                apply_adaptive_suppression=True
            )
            
            # Convert to numpy for ASR input
            mixed_features_np = mixed_features.cpu().numpy()
            enhanced_features_np = enhanced_features[0].cpu().numpy()
            
            # ASR evaluation (you would implement this with Whisper)
            # baseline_transcription = asr_system.transcribe(mixed_features_np)
            # enhanced_transcription = asr_system.transcribe(enhanced_features_np) 
            # wer_baseline = compute_wer(ground_truth, baseline_transcription)
            # wer_enhanced = compute_wer(ground_truth, enhanced_transcription)
            # wer_improvement = wer_baseline - wer_enhanced
            
            # For now, return feature quality as proxy
            feature_improvement = compute_feature_snr(
                enhanced_features, target_features.cuda(), mixed_features.cuda()
            )
            
            total_wer_improvement += feature_improvement
            num_samples += 1
    
    avg_improvement = total_wer_improvement / max(num_samples, 1)
    
    print(f"üéØ VoiceFilter-Lite 2020 ASR Evaluation:")
    print(f"   Average Feature Improvement: {avg_improvement:.2f} dB")
    print(f"   üìù TODO: Replace with actual Whisper WER evaluation")
    
    return avg_improvement
